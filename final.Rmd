---
title: "HES CSCI E-106 Statistical Data Modeling (Fall 2025 Semester)"
subtitle: "Titanic Survival Classification: Group 5 (Sigma 5) Project Report"
author:
- Aljazi Al Maghlouth
- Anjan Chakravarti
- Ganapathy Lakshmanaperumal
- Guy Nguyen-Phuoc
- Jonathan Terrasi
- Julie Lander
- Khatanbaatar Orkhon
- Max Amiesimaka
tags: [logistic regression, decision tree, classification]
abstract: |
  We build a champion/benchmark modeling solution to predict passenger
  survival on the RMS Titanic. The analysis covers exploratory data
  review, data preparation, model training, challenger comparison,
  performance evaluation, limitations, and monitoring guidance. All R
  code and interpretations are included for reproducibility and
  transparency.
date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: margin=1.3cm
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.path = "figures/",
  dev = "png"
)

library(tidyverse)
library(broom)
library(rpart)
library(rpart.plot)
library(MASS)
library(car)
library(caret)
library(pROC)
library(ResourceSelection)
library(vcd)

titanic_raw <- read_csv("Titanic_Survival_Data.csv", show_col_types = FALSE)
```

## Executive Summary

We predict Titanic passenger survival using demographic and ticketing
information. A cleaned dataset of 1,310 records is split 70/30
train/test (set.seed = 1023). The champion model is a parsimonious
logistic regression using class, sex, age, family size, fare, and port
of embarkation; a decision tree serves as the challenger. Both models
outperform chance; the logistic model delivers higher balanced accuracy
and interpretable odds ratios, while the tree offers simple rules but
slightly lower hold-out accuracy. Monitoring should track drift in class
mix, gender mix, and fare distributions, and trigger review when
accuracy drops below 0.80 or when inputs shift beyond training
percentiles. Key limitations include missing values (age, fare, cabin),
historical bias, and simplified imputations.

## I. Introduction (5 points)

This project classifies whether a passenger survived the Titanic
disaster using readily available features (class, sex, age, family
structure, fare, and port). We evaluate two supervised classification
methods: logistic regression (champion) and decision tree (challenger).
Success is defined by accurate and explainable survival predictions that
generalize to the hold-out test set.

## II. Description of the Data and Quality (15 points)

The dataset contains 1,310 observations and 14 original variables. Key
predictors are a mix of categorical (class, sex, embarked) and numeric
(age, fare, family counts). Several variables contain notable missing
values (age, cabin, boat, body, and home destination).

```{r data_overview, comment="", echo=FALSE}
glimpse(titanic_raw)

titanic_raw %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  pivot_longer(everything(),
               names_to = "variable",
               values_to = "n_missing") %>%
  arrange(desc(n_missing)) %>%
  knitr::kable(col.names = c("Variable", "Missing Count"))
```

### Data Preparation

We clean and engineer a modeling frame as follows:

1. Convert categorical variables to factors.
2. Impute age by sex/class median.
3. Impute fare with the overall median.
4. Drop high-missing columns.
5. Create a `family_size` helper feature.

```{r data_prep}
clean_titanic <- titanic_raw %>%
  mutate(
    survived = factor(survived, levels = c(0, 1),
                      labels = c("Died", "Survived")),
    pclass = factor(pclass, levels = c(1, 2, 3),
                    labels = c("1st", "2nd", "3rd")),
    sex = factor(sex),
    embarked = fct_explicit_na(embarked, "Unknown")
  )

age_medians <- clean_titanic %>%
  group_by(sex, pclass) %>%
  summarise(median_age = median(age, na.rm = TRUE), .groups = "drop")

clean_titanic <- clean_titanic %>%
  left_join(age_medians, by = c("sex", "pclass")) %>%
  mutate(
    age = ifelse(is.na(age), median_age, age),
    fare = ifelse(is.na(fare), median(fare, na.rm = TRUE), fare),
    family_size = sibsp + parch + 1
  ) %>%
  dplyr::select(survived, pclass, sex, age, sibsp, parch, family_size,
                fare, embarked)

summary(clean_titanic)
```

**Interpretation:** Imputation preserves sample size without extreme
values. Removing cabin/ticket/body/boat/home.dest reduces noise while
retaining predictive signal. The engineered `family_size` captures
non-linear survival dynamics for groups traveling together.

### Exploratory Graphs

```{r eda_plots, fig.width=7, fig.height=6}
clean_titanic %>%
  ggplot(aes(x = age, fill = survived)) +
  geom_histogram(position = "identity", alpha = 0.55, bins = 30) +
  labs(title = "Figure 1. Age distribution by survival",
       x = "Age", y = "Count") +
  theme_minimal()

clean_titanic %>%
  filter(!is.na(pclass), !is.na(survived)) %>%
  ggplot(aes(x = pclass, fill = survived)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "Figure 2. Survival share by passenger class",
    x = "Class", y = "Percent"
  ) +
  theme_minimal()

clean_titanic %>%
  filter(!is.na(sex), !is.na(survived)) %>%
  ggplot(aes(x = sex, fill = survived)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "Figure 3. Survival share by sex",
    x = "Sex", y = "Percent"
  ) +
  theme_minimal()
```

**Interpretation:** Survival probability is higher for younger
passengers, women, and higher classes. These patterns justify including
class, sex, and age in the model and suggest potential interactions.

## III. Model Development Process (15 points)

### Train/Test Split

```{r split}
set.seed(1023)
train_index <- sample(seq_len(nrow(clean_titanic)),
                      size = floor(0.7 * nrow(clean_titanic)))
titanic_train <- clean_titanic[train_index, ]
titanic_test  <- clean_titanic[-train_index, ]

table(titanic_train$survived)
table(titanic_test$survived)
```

**Interpretation:** The split preserves the original survival rate
(roughly 38% survived). Using a fixed seed allows reproducibility.

### Champion: Logistic Regression

```{r logit_model}
logit_model <- glm(
  survived ~ pclass + sex + age + family_size + embarked,
  data = titanic_train,
  family = binomial
)

tidy(logit_model, exponentiate = TRUE, conf.int = TRUE) %>%
  knitr::kable(
    digits = 3,
    col.names = c("Term", "Odds Ratio", "Std. Error", "z", "p-value",
                  "CI Lower", "CI Upper")
  )
```

**Interpretation:** Odds ratios show strong positive lift for females
and higher survival odds for 1st/2nd class. Increasing age slightly
decreases survival odds.

### Challenger: Decision Tree

```{r tree_model}
tree_model <- rpart(
  survived ~ pclass + sex + age + family_size + fare + embarked,
  data = titanic_train,
  method = "class",
  control = rpart.control(cp = 0.01, minsplit = 20)
)

rpart.plot(tree_model, main = "Figure 4. Decision tree challenger")
```

**Interpretation:** The tree yields intuitive rules (e.g., female and
1st- and 2nd-class passage leads to survival, whereas male and 3rd-class
passage has low survival). It trades probability granularity for
transparency.

## IV. Model Performance Testing (15 points)

### 4.1 Model Selection and Diagnostics

```{r stepwise_selection}
# Full logistic model
logit_full <- glm(
  survived ~ pclass + sex + age + family_size + fare + embarked,
  data = titanic_train,
  family = binomial
)

# Backward stepwise selection using AIC
logit_step <- stepAIC(logit_full, direction = "backward", trace = FALSE)

# Compare AIC values
cat("Full model AIC:", AIC(logit_full), "\n")
cat("Stepwise model AIC:", AIC(logit_step), "\n")
```

#### Multicollinearity (VIF)

```{r vif_check}
vif_values <- vif(logit_step)

vif_df <- if (is.matrix(vif_values)) {
  tibble::tibble(Predictor = rownames(vif_values),
                 VIF = vif_values[, 1])
} else {
  tibble::tibble(Predictor = names(vif_values),
                 VIF = as.numeric(vif_values))
}

vif_df %>%
  knitr::kable(digits = 2, caption = "Table 1: Variance Inflation Factors")
```

#### Linearity of the Logit (Box-Tidwell)

```{r box_tidwell}
titanic_train_bt <- titanic_train %>%
  mutate(
    age_log = age * log(age + 1),
    fare_log = fare * log(fare + 1),
    family_size_log = family_size * log(family_size + 1)
  )

logit_bt <- glm(
  survived ~ pclass + sex + age + family_size + fare + embarked +
    age_log + fare_log + family_size_log,
  data = titanic_train_bt,
  family = binomial
)

tidy(logit_bt) %>%
  filter(term %in% c("age_log", "fare_log", "family_size_log")) %>%
  dplyr::select(term, estimate, p.value) %>%
  knitr::kable(digits = 4,
               caption = "Table 2: Box-Tidwell Linearity Test")
```

#### Influential Observations (Cook's Distance)

```{r  cooks_distance, fig.width=7, fig.height=4, fig.align='left'}
cooksd <- cooks.distance(logit_step)

plot(cooksd, pch = 20,
     main = "Figure 5. Cook's Distance for Influential Observations",
     ylab = "Cook's Distance", xlab = "Observation Index")
abline(h = 4 / nrow(titanic_train), col = "red", lty = 2)
```

### 4.2 Test Set Performance

```{r test_predictions}
# Logistic regression predictions
logit_pred_prob <- predict(logit_step, newdata = titanic_test, type = "response")
logit_pred_class <- ifelse(logit_pred_prob > 0.5, "Survived", "Died")
logit_pred_class <- factor(logit_pred_class, levels = c("Died", "Survived"))

# Decision tree predictions
tree_pred_class <- predict(tree_model, newdata = titanic_test, type = "class")
```

#### Confusion Matrices

```{r confusion_matrices}
logit_cm <- confusionMatrix(logit_pred_class, titanic_test$survived, positive = "Survived")
tree_cm  <- confusionMatrix(tree_pred_class, titanic_test$survived,  positive = "Survived")

logit_cm$table
logit_cm$overall
logit_cm$byClass

tree_cm$table
tree_cm$overall
tree_cm$byClass
```

#### Performance Metrics Comparison

```{r metrics_comparison, fig.width=7, fig.height=6, fig.align='left'}
metrics_comparison <- data.frame(
  Model = c("Logistic Regression", "Decision Tree"),
  Accuracy = c(logit_cm$overall["Accuracy"], tree_cm$overall["Accuracy"]),
  Sensitivity = c(logit_cm$byClass["Sensitivity"], tree_cm$byClass["Sensitivity"]),
  Specificity = c(logit_cm$byClass["Specificity"], tree_cm$byClass["Specificity"]),
  Precision = c(logit_cm$byClass["Precision"], tree_cm$byClass["Precision"]),
  F1_Score = c(logit_cm$byClass["F1"], tree_cm$byClass["F1"]),
  Balanced_Accuracy = c(logit_cm$byClass["Balanced Accuracy"],
                        tree_cm$byClass["Balanced Accuracy"])
)

metrics_comparison %>%
  knitr::kable(digits = 4,
               caption = "Table 3: Test Set Performance Comparison")
```

#### ROC Curve and AUC

```{r roc_auc, fig.width=7, fig.height=6, fig.align='left'}
logit_roc <- roc(titanic_test$survived, logit_pred_prob, levels = c("Died", "Survived"))
logit_auc <- auc(logit_roc)

tree_pred_prob <- predict(tree_model, newdata = titanic_test, type = "prob")[, "Survived"]
tree_roc <- roc(titanic_test$survived, tree_pred_prob, levels = c("Died", "Survived"))
tree_auc <- auc(tree_roc)

plot(logit_roc, col = "blue", lwd = 2,
     main = "Figure 6. ROC Curves: Model Comparison")
plot(tree_roc, col = "red", lwd = 2, add = TRUE)
legend("bottomright",
       legend = c(paste("Logistic Regression (AUC =", round(logit_auc, 3), ")"),
                  paste("Decision Tree (AUC =", round(tree_auc, 3), ")")),
       col = c("blue", "red"), lwd = 2)
```

#### Goodness-of-Fit

```{r goodness_of_fit}
# Pseudo R-squared (McFadden)
logit_null <- glm(survived ~ 1, data = titanic_train, family = binomial)
pseudo_r2 <- 1 - (as.numeric(logLik(logit_step)) / as.numeric(logLik(logit_null)))

# Hosmer-Lemeshow test using model-fitted response (matching lengths)
hl_df <- data.frame(
  y = as.numeric(logit_step$y),
  yhat = as.numeric(fitted(logit_step))
)
hl_df <- na.omit(hl_df)

hl_test <- hoslem.test(hl_df$y, hl_df$yhat, g = 10)

cat("McFadden's Pseudo R^2:", round(pseudo_r2, 4), "\n")
cat("Hosmer-Lemeshow p-value:", round(hl_test$p.value, 4), "\n")
```

#### Residual Analysis

```{r residual_plot, fig.width=7, fig.height=5, fig.align='left'}
residuals_dev <- residuals(logit_step, type = "deviance")

plot(fitted(logit_step), residuals_dev,
     pch = 20, col = scales::alpha("black", 0.5),
     xlab = "Fitted Values (Log-Odds)", ylab = "Deviance Residuals",
     main = "Figure 7. Deviance Residuals vs. Fitted Values")
abline(h = 0, col = "red", lty = 2)
```

### 4.3 Champion Model Summary

```{r champion_summary}
cat("=== CHAMPION MODEL SUMMARY ===\n\n")
cat("Model Type: Logistic Regression (Stepwise Selected)\n")
cat("Test Accuracy:", round(logit_cm$overall["Accuracy"], 4), "\n")
cat("Test AUC:", round(logit_auc, 4), "\n")
cat("Pseudo R^2:", round(pseudo_r2, 4), "\n")
cat("Multicollinearity: VIF values <", round(max(vif_values), 2), "\n")
cat("Linearity of Logit: Assessed via Box-Tidwell\n")
cat("Goodness-of-Fit: Hosmer-Lemeshow p-value =", round(hl_test$p.value, 4), "\n\n")

tidy(logit_step, exponentiate = TRUE, conf.int = TRUE) %>%
  knitr::kable(digits = 3,
               col.names = c("Term", "Odds Ratio", "Std. Error",
                             "z", "p-value", "CI Lower", "CI Upper"))
```

## V. Challenger Models (15 points)

- Decision tree (rpart) built with the same predictors.
- Provides transparent decision rules but slightly lower AUC/accuracy.
- Useful as an audit-friendly benchmark against the logistic regression.

## VI. Model Limitations and Assumptions (15 points)

- Missing data handled with median imputations; alternative methods
  (e.g., multiple imputation) could shift coefficients.  
- Model assumes stability of relationships over time; historical bias
  may limit portability to other contexts.  
- Logistic regression assumes linearity in the log-odds for numeric
  predictors and absence of strong multicollinearity.  
- Outliers may influence coefficients despite Cook's distance checks.

## VII. Ongoing Model Monitoring Plan (5 points)

- **Data drift:** Track distributions of `pclass`, `sex`, `fare`, and
  `family_size`; trigger review if shifts exceed training
  5th/95th percentiles.  
- **Performance:** Recompute accuracy, balanced accuracy, and AUC
  quarterly; retrain if accuracy < 0.80 or AUC < 0.78.  
- **Stability:** Monitor calibration (Hosmer-Lemeshow) and confusion
  matrix balance; investigate rising false negatives (missed survivors).  
- **Process:** Freeze scoring code, log model version/seed, and maintain
  challenger comparisons on new data.

## VIII. Conclusion (5 points)

The stepwise logistic regression is the champion model: it delivers
strong discriminatory power, balanced performance, and interpretable
odds ratios. The decision tree serves as a transparent benchmark but
trails slightly in AUC and accuracy. Monitoring should focus on input
drift and sustained predictive performance to ensure continued fitness
for purpose.

This project developed and evaluated statistical classification models to predict passenger",
"survival on the RMS Titanic. After a comprehensive analysis of data quality, exploratory",
"patterns, model diagnostics, and test-set performance, the stepwise logistic regression",
"model emerged as the champion model. Its strengths include stable and interpretable",
"coefficients, strong discriminatory power (AUC = 0.81), balanced accuracy, and robustness",
"against multicollinearity. The model effectively captured key survival determinants such as",
"passenger class, sex, age, family structure, and port of embarkation.

The decision tree served as a transparent challenger model, offering rule-based explanations",
"that align with well-known Titanic survival dynamics (e.g., higher survival rates among women",
"and passengers in first class). Although the tree performed competitively with accuracy and",
"AUC close to the logistic regression it exhibited slightly lower generalization performance on",
"the hold-out test set, justifying its role as a benchmark rather than the primary model.

Overall, the modeling framework demonstrates that demographic and ticket-related features",
"provide meaningful predictive signal for survival classification. The results highlight the",
"importance of rigorous feature engineering, proper handling of missing data, and balanced",
"evaluation across accuracy, sensitivity, specificity, and AUC. While the champion model is well",
"suited for this dataset, limitations such as historical bias, imputation uncertainty, and",
"non-linear effects suggest opportunities for future enhancement using ensemble methods or",
"calibrated probability models.

The final recommendation is to adopt the logistic regression model as the primary forecasting",
"tool, supported by ongoing monitoring of input drift, predictive performance decay, and model",
"assumptions. With appropriate governance, this modeling solution is fit for purpose and",
"provides a reproducible, transparent, and statistically grounded approach to survival",
"prediction on the Titanic dataset.

```{r  comment="", echo=FALSE}

```
## Bibliography (7 points)

Kaggle. (n.d.). *Titanic: Machine Learning from Disaster*. Retrieved from [https://www.kaggle.com/competitions/titanic](https://www.kaggle.com/competitions/titanic). Used as the primary source of the Titanic survival dataset and baseline feature descriptions.

James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). *An Introduction to Statistical Learning with Applications in R* (2nd ed.). Springer. Key reference for logistic regression, model selection, and performance evaluation concepts applied in this report.

Kuhn, M., & Johnson, K. (2013). *Applied Predictive Modeling*. Springer. Used for guidance on train/test splitting, confusion matrices, ROC curves, AUC, and model comparison in classification problems.

Hosmer, D. W., Lemeshow, S., & Sturdivant, R. X. (2013). *Applied Logistic Regression* (3rd ed.). Wiley. Reference for interpreting odds ratios, assessing goodness-of-fit, and applying the Hosmer–Lemeshow test for the champion logistic model.

Harrell, F. E. (2015). *Regression Modeling Strategies* (2nd ed.). Springer. Conceptual support for model building, handling of nonlinearity, and the use of diagnostic plots for residuals and influential observations.

Kuhn, M. (2008). *caret: Classification and Regression Training* [R package]. Comprehensive R framework used for confusion matrices and model performance metrics; see CRAN documentation for implementation details.

Robin, X., et al. (2011). pROC: an open-source package for R and S+ to analyze and compare ROC curves. *BMC Bioinformatics*, 12(77). Used for ROC curve plotting and AUC computation for the logistic regression and decision tree models.

Therneau, T., & Atkinson, B. (2024). *rpart: Recursive Partitioning and Regression Trees* [R package]. Implemented the decision tree challenger model for survival classification and rule-based interpretation.

Milborrow, S. (2024). *rpart.plot: Plot 'rpart' Models* [R package]. Used to visualize the decision tree structure and communicate simple, interpretable survivor/non-survivor rules.

R Core Team. (2024). *R: A Language and Environment for Statistical Computing*. R Foundation for Statistical Computing, Vienna, Austria. The statistical computing environment used for all data manipulation, modeling, and graphics in this project.

## Appendix (3 points)

### A1. Additional Exploratory Data Analysis
```{r comment="", echo=FALSE}
clean_titanic %>%
  filter(!is.na(survived), !is.na(age)) %>%
  ggplot(aes(x = survived, y = age, fill = survived)) +
  geom_boxplot(alpha = 0.7) +
  labs(
    title = "Appendix Figure A1. Age by Survival Status",
    x = "Survival", 
    y = "Age"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

```

```{r comment="", echo=FALSE}
clean_titanic %>%
  filter(!is.na(survived), !is.na(fare)) %>%
  ggplot(aes(x = survived, y = fare, fill = survived)) +
  geom_boxplot(alpha = 0.7) +
  coord_cartesian(
    ylim = c(
      0,
      quantile(clean_titanic$fare, 0.95, na.rm = TRUE)
    )
  ) +
  labs(
    title = "Appendix Figure A2. Fare by Survival Status (Truncated at 95th Percentile)",
    x = "Survival", 
    y = "Fare"
  ) +
  theme_minimal() +
  theme(legend.position = "none")

```

```{r comment="", echo=FALSE}
clean_titanic %>%
  filter(!is.na(pclass), !is.na(sex), !is.na(survived)) %>%
  count(pclass, sex, survived) %>%
  ggplot(aes(x = pclass, y = n, fill = survived)) +
  geom_col(position = "fill") +
  facet_wrap(~ sex) +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(
    title = "Appendix Figure A3. Survival Share by Class and Sex",
    x = "Passenger Class", 
    y = "Percent"
  ) +
  theme_minimal()

```

```{r comment="", echo=FALSE}
num_vars <- clean_titanic %>%
dplyr::select(age, sibsp, parch, family_size, fare) %>%
na.omit()

corr_mat <- cor(num_vars)

corr_df <- as.data.frame(as.table(corr_mat))
names(corr_df) <- c("Var1", "Var2", "Correlation")

ggplot(corr_df, aes(x = Var1, y = Var2, fill = Correlation)) +
geom_tile() +
scale_fill_gradient2(limits = c(-1, 1)) +
labs(title = "Appendix Figure A4. Correlation Heatmap of Numeric Predictors",
x = "", y = "") +
theme_minimal()

```

```{r comment="", echo=FALSE}
comment_text <- paste(
"Interpretation (EDA): The additional plots confirm strong differences",
"in age and fare distributions across survival groups and highlight",
"interaction patterns between class and sex. The correlation heatmap",
"shows only moderate correlations among numeric predictors, which is",
"consistent with the low VIF values reported in the main text."
)

cat(strwrap(comment_text, width = 70), sep = "\n")

```

### A2. Detailed Confusion Matrices and Threshold Sensitivity
```{r comment="", echo=FALSE}
if (!require(knitr)) install.packages("knitr", repos="https://cloud.r-project.org")
library(knitr)
# Recreate predictions if needed

logit_pred_prob <- predict(logit_step, newdata = titanic_test, type = "response")
logit_pred_class <- ifelse(logit_pred_prob > 0.5, "Survived", "Died")
logit_pred_class <- factor(logit_pred_class, levels = c("Died", "Survived"))

tree_pred_class <- predict(tree_model, newdata = titanic_test, type = "class")

logit_tab <- table(Prediction = logit_pred_class, Reference = titanic_test$survived)
tree_tab  <- table(Prediction = tree_pred_class,  Reference = titanic_test$survived)

kable(logit_tab, caption = "Appendix Table A1. Confusion Matrix – Logistic Regression (Cutoff = 0.50)")
kable(tree_tab,  caption = "Appendix Table A2. Confusion Matrix – Decision Tree")

```

```{r comment="", echo=FALSE}
threshold_grid <- seq(0.2, 0.8, by = 0.05)

calc_metrics <- function(thresh) {
pred_class <- ifelse(logit_pred_prob > thresh, "Survived", "Died")
pred_class <- factor(pred_class, levels = c("Died", "Survived"))
tab <- table(pred_class, titanic_test$survived)

TP <- tab["Survived", "Survived"]
TN <- tab["Died",     "Died"]
FP <- tab["Survived", "Died"]
FN <- tab["Died",     "Survived"]

acc  <- (TP + TN) / sum(tab)
sens <- TP / (TP + FN)
spec <- TN / (TN + FP)

data.frame(threshold = thresh,
accuracy  = acc,
sensitivity = sens,
specificity = spec)
}

threshold_df <- do.call(rbind, lapply(threshold_grid, calc_metrics))

threshold_df %>%
pivot_longer(cols = c("accuracy", "sensitivity", "specificity"),
names_to = "metric", values_to = "value") %>%
ggplot(aes(x = threshold, y = value, linetype = metric)) +
geom_line() +
ylim(0, 1) +
labs(title = "Appendix Figure A5. Logistic Regression: Threshold Sensitivity",
x = "Classification Threshold", y = "Metric Value") +
theme_minimal()

```

```{r comment="", echo=FALSE}
comment_text <- paste(
"Interpretation (Thresholds): Varying the cutoff between 0.20 and 0.80",
"shows the usual trade-off between sensitivity and specificity. The",
"default 0.50 threshold achieves a good balance, but alternative",
"cutoffs could be chosen if business priorities required fewer false",
"negatives or fewer false positives."
)

cat(strwrap(comment_text, width = 70), sep = "\n")

```

### A3. Additional ROC / AUC Details
```{r comment="", echo=FALSE}
logit_roc <- roc(titanic_test$survived, logit_pred_prob, levels = c("Died", "Survived"))
logit_auc <- auc(logit_roc)

tree_pred_prob <- predict(tree_model, newdata = titanic_test, type = "prob")[, "Survived"]
tree_roc <- roc(titanic_test$survived, tree_pred_prob, levels = c("Died", "Survived"))
tree_auc <- auc(tree_roc)

roc_df <- data.frame(
Model = c("Logistic Regression", "Decision Tree"),
AUC   = c(as.numeric(logit_auc), as.numeric(tree_auc))
)

kable(roc_df, digits = 3,
caption = "Appendix Table A3. Area Under the ROC Curve (AUC) by Model")

```

```{r comment="", echo=FALSE}
comment_text <- paste(
"Interpretation (AUC): Both models substantially outperform random",
"classification, with AUC values around 0.80. The logistic regression",
"shows a slightly higher AUC than the decision tree, which supports its",
"selection as the champion model in the main text."
)

cat(strwrap(comment_text, width = 70), sep = "\n")

```

### A4. Reproducibility Notes
```{r comment="", echo=FALSE}
comment_text <- paste(
"Reproducibility: All results in the report can be regenerated by",
"running this R Markdown document from top to bottom. Key modeling",
"choices include the 70/30 train/test split with set.seed(1023),",
"median imputations for age and fare, and the predictor set used in",
"the logistic regression and decision tree. This appendix collects",
"supporting plots and tables that were omitted from the main body",
"for brevity but may be helpful for technical reviewers."
)

cat(strwrap(comment_text, width = 70), sep = "\n")

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```

```{r}

```
