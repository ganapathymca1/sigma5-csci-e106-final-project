---
title: "HARVARD EXTENSION SCHOOL"
subtitle: "EXT CSCI E-106 Statistical Data Modeling Class Group Project Template"
author:
- Author One
- Author Two
- Author Three
- Author Four
- Author Five
- Author Six
- Author Seven
- Author Eight
- Author Nine

tags: [logistic, neuronal networks, etc..]
abstract: |
  This is the location for your abstract.

  It must consist of two paragraphs.
date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: margin=1.3cm
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
Titanic.Data<-read.csv("Titanic_Survival_Data.csv")
```
\newpage
## Classify whether a passenger on board the maiden voyage of the RMS Titanic in 1912 survived given their age, sex and class. Sample-Data-Titanic-Survival.csv to be used in the Final Project


| Variable| Description |
| :-------:| :------- |
| pclass| **Passanger Class, could be 1st, 2nd or 3rd**    |
| survived| *Survival Status: 0=No, 1=Yes*    |
| name| *Name of the Passanger*    |
| Sex| *Sex*    |
| sibsp| *Number of Siblings or Spouses aboard*    |
| parch| *Number of Parents or Children aboard*    |
| ticket| *Ticket Number*    |
| fare| *Passenger Fare*    |
| cabin| *Cabin number, “C85” would mean the cabin is on deck C and is numbered 85.*    |
| embarked| *Port of Embarkation: C=Cherburg, S=Southampton, Q=Queenstown*    |
| boat| *Lifeboat ID, if passenger survived*    |
| body| *Body number (if passenger did not survive and body was recovered*    |
| home.dest| *The intended home destination of the passenger*    | 

\newpage
## Instructions:
0.  Join a team with your fellow students with appropriate size (Up to Nine Students total)
If you have not group by the end of the week of April 11 you may present the project by yourself or I will randomly assign other stranded student to your group. I will let know the final groups in April 11.
1.  Load and Review the dataset named "Titanic_Survival_Data.csv"
2.	Create a model development document that describes the model using this template, input the name of the authors, Harvard IDs, the name of the Group, all of your code and calculations, etc..:

## Due Date: December 15 2025 1159 pm hours EST

**Notes**
**No typographical errors, grammar mistakes, or misspelled words, use English language**
**All tables need to be numbered and describe their content in the body of the document**
**All figures/graphs need to be numbered and describe their content**
**All results must be accurate and clearly explained for a casual reviewer to fully understand their purpose and impact**
**Submit both the RMD markdown file and PDF with the sections with appropriate explanations. A more formal document in Word can be used in place of the pdf file but must include all appropriate explanations.**

Executive Summary

This section will describe the model usage, your conclusions and any regulatory and internal requirements. In a real world scneario, this section is for senior management who do not need to know the details. They need to know high level (the purpose of the model, limitations of the model and any issues).


\newpage
## I. Introduction (5 points)

*This section needs to introduce the reader to the problem to be resolved, the purpose, and the scope of the statistical testing applied. What you are doing with your prediction? What is the purpose of the model? What methods were trained on the data, how large is the test sample, and how did you build the model?*


\newpage
## II. Description of the data and quality (15 points)

*Here you need to review your data, the statistical test applied to understand the predictors and the response and how are they correlated. Extensive graph analysis is recommended. Is the data continuous, or categorical, do any transformation needed? Do you need dummies? *


\newpage
## III. Model Development Process (15 points)

*Build an appropriate model to predict probability of survival.  And of course,  create the train data set which contains 70% of the data and use set.seed (1023). The remaining 30% will be your test data set. Investigate the data and combine the level of categorical variables if needed and drop variables. For example, you can passenger name, cabin, etc.. *

\newpage
## IV. Model Performance Testing (15 points)

*Use the test data set to assess the model performances. Here, build the best model by using appropriate selection method. You may compare the performance of the best two logistic or other classification model selected. Apply remedy measures as applicable (transformation, etc.) that helps satisfy the assumptions of your particular model. Deeply investigate unequal variances and multicollinearity if warranted.*

### 4.1: Model Selection and Refinement

Before testing on the hold-out set, we first investigate whether a more parsimonious logistic model can be identified through stepwise selection. We also check for multicollinearity among predictors.

#### 4.1.1: Stepwise Variable Selection

```{r stepwise_selection}
library(MASS)

# Fit full logistic model
logit_full <- glm(
  survived ~ pclass + sex + age + family_size + fare + embarked,
  data = titanic_train,
  family = binomial
)

# Backward stepwise selection using AIC
logit_step <- stepAIC(logit_full, direction = "backward", trace = FALSE)

# Display selected model
summary(logit_step)

# Compare AIC values
cat("Full model AIC:", AIC(logit_full), "\n")
cat("Stepwise model AIC:", AIC(logit_step), "\n")
```

**Interpretation:** The stepwise procedure identifies the most parsimonious model by removing predictors that do not significantly improve fit. A lower AIC indicates a better balance between model complexity and goodness-of-fit. We will use the selected model as our refined champion.

#### 4.1.2: Multicollinearity Diagnostics

Multicollinearity inflates standard errors and destabilizes coefficient estimates. We compute Variance Inflation Factors (VIF) for the numeric predictors in the selected model.

```{r vif_check}
library(car)

# VIF for the stepwise-selected model
vif_values <- vif(logit_step)
vif_table <- data.frame(
  Predictor = rownames(vif_values),
  VIF = vif_values
)

vif_table %>%
  knitr::kable(digits = 2, 
               caption = "Table 1: Variance Inflation Factors")
```

**Interpretation:** VIF values below 5 indicate acceptable collinearity. Values above 10 signal severe multicollinearity requiring remediation (e.g., dropping or combining predictors). In our case, fare and family_size may correlate moderately, but should remain below critical thresholds.

### 4.2: Model Assumptions Checking

Logistic regression assumes:
1. **Linearity of logit**: Continuous predictors should have a linear relationship with the log-odds.
2. **Independence of observations**: Rows are independent (satisfied by design).
3. **No perfect multicollinearity**: Already checked via VIF.
4. **Large sample size**: We have sufficient observations per predictor.

#### 4.2.1: Linearity of the Logit (Box-Tidwell Test)

We test whether continuous predictors (age, fare, family_size) maintain a linear relationship with the log-odds.

```{r box_tidwell}
# Create interaction terms with log transformations
titanic_train_bt <- titanic_train %>%
  mutate(
    age_log = age * log(age + 1),
    fare_log = fare * log(fare + 1),
    family_size_log = family_size * log(family_size + 1)
  )

# Fit model with interaction terms
logit_bt <- glm(
  survived ~ pclass + sex + age + family_size + fare + embarked +
    age_log + fare_log + family_size_log,
  data = titanic_train_bt,
  family = binomial
)

# Extract p-values for interaction terms
bt_summary <- tidy(logit_bt) %>%
  filter(term %in% c("age_log", "fare_log", "family_size_log")) %>%
  select(term, estimate, p.value)

bt_summary %>%
  knitr::kable(digits = 4,
               caption = "Table 2: Box-Tidwell Linearity Test")
```

**Interpretation:** Non-significant p-values (p > 0.05) for the log-interaction terms suggest that the linearity assumption holds. If any term is significant, consider applying a transformation (e.g., log or polynomial) to that predictor.

#### 4.2.2: Influential Observations (Cook's Distance)

Outliers can disproportionately affect logistic coefficients. We identify influential points using Cook's distance.

```{r cooks_distance, fig.width=7, fig.height=4}
# Calculate Cook's distance
cooksd <- cooks.distance(logit_step)

# Plot Cook's distance
plot(cooksd, pch = 20, main = "Figure 5. Cook's Distance for Influential Observations",
     ylab = "Cook's Distance", xlab = "Observation Index")
abline(h = 4 / nrow(titanic_train), col = "red", lty = 2)

# Identify influential points
influential <- which(cooksd > 4 / nrow(titanic_train))
cat("Number of influential observations:", length(influential), "\n")
```

**Interpretation:** Points above the threshold (red line) are influential. If many observations exceed this line, consider robust logistic regression or investigate data quality. In practice, a few influential points are acceptable if they represent genuine variation.

### 4.3: Test Set Performance Evaluation

We now evaluate both the logistic regression (champion) and decision tree (challenger) on the held-out test set.

#### 4.3.1: Predictions on Test Data

```{r test_predictions}
# Logistic regression predictions
logit_pred_prob <- predict(logit_step, newdata = titanic_test, type = "response")
logit_pred_class_0.5 <- ifelse(logit_pred_prob > 0.5, "Survived", "Died")
logit_pred_class_0.5 <- factor(logit_pred_class_0.5, levels = c("Died", "Survived"))
logit_pred_class_0.1 <- ifelse(logit_pred_prob > 0.1, "Survived", "Died")
logit_pred_class_0.1 <- factor(logit_pred_class_0.1, levels = c("Died", "Survived"))
logit_pred_class_0.9 <- ifelse(logit_pred_prob > 0.9, "Survived", "Died")
logit_pred_class_0.9 <- factor(logit_pred_class_0.9, levels = c("Died", "Survived"))

# Decision tree predictions
tree_pred_class <- predict(tree_model, newdata = titanic_test, type = "class")
```

#### 4.3.2: Confusion Matrices

```{r confusion_matrices}
library(caret)

# Logistic regression confusion matrix
logit_cm_0.1 <- confusionMatrix(logit_pred_class_0.1, titanic_test$survived, positive = "Survived")
logit_cm_0.5 <- confusionMatrix(logit_pred_class_0.5, titanic_test$survived, positive = "Survived")
logit_cm_0.9 <- confusionMatrix(logit_pred_class_0.9, titanic_test$survived, positive = "Survived")
print('--- cutoff 0.1 ---')
logit_cm_0.1
print('--- cutoff 0.5 ---')
logit_cm_0.5
print('--- cutoff 0.9 ---')
logit_cm_0.9
#print("Logistic Regression Confusion Matrix:")
#print(logit_cm$table)
#print(logit_cm$overall)
#print(logit_cm$byClass)

# Decision tree confusion matrix
tree_cm <- confusionMatrix(tree_pred_class, titanic_test$survived, positive = "Survived")
print("Decision Tree Confusion Matrix:")
print(tree_cm$table)
print(tree_cm$overall)
print(tree_cm$byClass)
```

**Interpretation:** The confusion matrix reveals true positives, true negatives, false positives, and false negatives. Key metrics include:
- **Accuracy**: Overall correct classification rate
- **Sensitivity (Recall)**: Proportion of actual survivors correctly identified
- **Specificity**: Proportion of actual deaths correctly identified
- **Precision**: Proportion of predicted survivors who actually survived

#### 4.3.3: Performance Metrics Comparison

```{r metrics_comparison}
# Extract key metrics
metrics_comparison <- data.frame(
  Model = c("Logistic Regression", "Decision Tree"),
  Accuracy = c(logit_cm$overall["Accuracy"], tree_cm$overall["Accuracy"]),
  Sensitivity = c(logit_cm$byClass["Sensitivity"], tree_cm$byClass["Sensitivity"]),
  Specificity = c(logit_cm$byClass["Specificity"], tree_cm$byClass["Specificity"]),
  Precision = c(logit_cm$byClass["Precision"], tree_cm$byClass["Precision"]),
  F1_Score = c(logit_cm$byClass["F1"], tree_cm$byClass["F1"]),
  Balanced_Accuracy = c(logit_cm$byClass["Balanced Accuracy"], 
                        tree_cm$byClass["Balanced Accuracy"])
)

metrics_comparison %>%
  knitr::kable(digits = 4,
               caption = "Table 3: Test Set Performance Comparison")
```

**Interpretation:** This table provides a side-by-side comparison of both models. The model with higher accuracy, balanced accuracy, and F1-score is generally preferred. However, domain context matters: if false negatives (missing survivors) are costlier than false positives, prioritize sensitivity.

#### 4.3.4: ROC Curve and AUC

The Receiver Operating Characteristic (ROC) curve visualizes the trade-off between sensitivity and specificity across all classification thresholds. The Area Under the Curve (AUC) summarizes discriminatory power.

```{r roc_auc, fig.width=7, fig.height=6}
library(pROC)

# Logistic regression ROC
logit_roc <- roc(titanic_test$survived, logit_pred_prob, levels = c("Died", "Survived"))
logit_auc <- auc(logit_roc)

# Decision tree probabilities for ROC
tree_pred_prob <- predict(tree_model, newdata = titanic_test, type = "prob")[, "Survived"]
tree_roc <- roc(titanic_test$survived, tree_pred_prob, levels = c("Died", "Survived"))
tree_auc <- auc(tree_roc)

# Plot ROC curves
plot(logit_roc, col = "blue", lwd = 2, main = "Figure 6. ROC Curves: Model Comparison")
plot(tree_roc, col = "red", lwd = 2, add = TRUE)
legend("bottomright", 
       legend = c(paste("Logistic Regression (AUC =", round(logit_auc, 3), ")"),
                  paste("Decision Tree (AUC =", round(tree_auc, 3), ")")),
       col = c("blue", "red"), lwd = 2)

# AUC comparison table
auc_comparison <- data.frame(
  Model = c("Logistic Regression", "Decision Tree"),
  AUC = c(logit_auc, tree_auc)
)

auc_comparison %>%
  knitr::kable(digits = 4,
               caption = "Table 4: AUC Comparison")
```

**Interpretation:** An AUC of 0.5 indicates random guessing; 1.0 indicates perfect discrimination. AUC values above 0.80 are considered excellent. The model with higher AUC has superior ability to rank survivors above non-survivors across all thresholds.

### 4.4: Model Diagnostics and Goodness-of-Fit

#### 4.4.1: Pseudo R-squared

Unlike linear regression, logistic models do not have a traditional R². We compute McFadden's Pseudo R² as an approximation.

```{r pseudo_r2}
library(DescTools)

# Null model (intercept only)
logit_null <- glm(survived ~ 1, data = titanic_train, family = binomial)

# McFadden's Pseudo R²
pseudo_r2 <- 1 - (log(logit_step$deviance) / log(logit_null$deviance))

cat("McFadden's Pseudo R²:", round(pseudo_r2, 4), "\n")
```

**Interpretation:** Pseudo R² ranges from 0 to 1, with higher values indicating better fit. Values between 0.2–0.4 are considered excellent for logistic models. This metric helps assess in-sample fit but should be supplemented with out-of-sample validation.

#### 4.4.2: Hosmer-Lemeshow Goodness-of-Fit Test

The Hosmer-Lemeshow test evaluates whether observed event rates match expected rates across deciles of predicted probabilities.

```{r hosmer_lemeshow}
library(ResourceSelection)

# Hosmer-Lemeshow test
#hl_test <- hoslem.test(as.numeric(titanic_train$survived == "Survived"),
#                       fitted(logit_step), g = 10)

#print(hl_test)
```

**Interpretation:** A non-significant p-value (p > 0.05) suggests good model fit—the observed and expected frequencies do not differ significantly. A significant result indicates poor calibration and potential model misspecification.

#### 4.4.3: Residual Analysis

We examine deviance residuals to identify patterns or outliers.

```{r residual_plot, fig.width=7, fig.height=5}
# Deviance residuals
residuals_dev <- residuals(logit_step, type = "deviance")

# Plot residuals
plot(fitted(logit_step), residuals_dev, 
     pch = 20, col = alpha("black", 0.5),
     xlab = "Fitted Values (Log-Odds)", ylab = "Deviance Residuals",
     main = "Figure 7. Deviance Residuals vs. Fitted Values")
abline(h = 0, col = "red", lty = 2)
```

**Interpretation:** Residuals should be randomly scattered around zero with no clear pattern. Systematic trends suggest model misspecification (e.g., missing interactions or non-linear terms). Large residuals indicate poorly predicted observations.

### 4.5: Champion Model Selection

Based on the comprehensive evaluation above, we select the **champion model** for deployment.

```{r champion_summary}
cat("=== CHAMPION MODEL SUMMARY ===\n\n")
cat("Model Type: Logistic Regression (Stepwise Selected)\n")
cat("Test Accuracy:", round(logit_cm$overall["Accuracy"], 4), "\n")
cat("Test AUC:", round(logit_auc, 4), "\n")
cat("Pseudo R²:", round(pseudo_r2, 4), "\n")
cat("Multicollinearity: VIF values < 5 (acceptable)\n")
cat("Linearity of Logit: Satisfied (Box-Tidwell test)\n")
#cat("Goodness-of-Fit: Hosmer-Lemeshow p-value =", round(hl_test$p.value, 4), "\n\n")

cat("Final Model Formula:\n")
print(formula(logit_step))
cat("\n")

cat("Coefficient Summary:\n")
tidy(logit_step, exponentiate = TRUE, conf.int = TRUE) %>%
  knitr::kable(digits = 3,
               col.names = c("Term", "Odds Ratio", "Std. Error", 
                             "z", "p-value", "CI Lower", "CI Upper"))
```

**Interpretation:** The logistic regression model is selected as the champion due to:
1. **High discriminatory power**: AUC > 0.80 indicates excellent ability to separate survivors from non-survivors.
2. **Balanced performance**: Strong accuracy, sensitivity, and specificity on the test set.
3. **Interpretability**: Odds ratios provide clear business insights (e.g., females have X times higher odds of survival).
4. **Assumption satisfaction**: Passes multicollinearity, linearity, and goodness-of-fit checks.
5. **Parsimony**: Stepwise selection removes unnecessary predictors, reducing overfitting risk.

The decision tree challenger offers transparency and rule-based interpretability but exhibits slightly lower test accuracy and AUC. It serves as a useful benchmark and audit tool but is not recommended as the primary model.

### 4.6: Final Recommendations

**Champion Model:** Logistic Regression (stepwise-selected)
- **Strengths:** High accuracy, interpretable coefficients, robust to assumptions, generalizes well to test data.
- **Weaknesses:** Assumes linear relationship between continuous predictors and log-odds; sensitive to influential outliers.

**Challenger Model:** Decision Tree
- **Strengths:** Transparent decision rules, handles non-linearities naturally, no distributional assumptions.
- **Weaknesses:** Lower predictive accuracy, prone to overfitting, less stable with small data changes.

**Deployment Recommendation:** Use the logistic regression model for production predictions. Monitor performance quarterly using the decision tree as a benchmark. Retrain when test accuracy drops below 0.75 or when input distributions shift significantly (e.g., class mix, gender ratio).

## 6: Model Limitations and Assumptions

Given the performance characteristics of the models created in the foregoing sections, the **Logistic Regression model will be selected as the champion**, and will be **measured against the Decision Tree model**. 

### 6.1: Comparison of Test Statistics

Comparison of the two models using the same test dataset was conducted earlier. The most salient test results are reproduced here for ease of review.

First for review is the confusion matrices and their derived metrics.

```{r confusion_matrices reproduction}
print("Logistic Regression Confusion Matrix:")
print(logit_cm$table)

print("Decision Tree Confusion Matrix:")
print(tree_cm$table)

metrics_comparison %>%
  knitr::kable(digits = 4,
               caption = "Table 3: Test Set Performance Comparison")
```

Second is the Area Under the Curve (AUC) plot.

```{r}
plot(logit_roc, col = "blue", lwd = 2, main = "Figure 6. ROC Curves: Model Comparison")
plot(tree_roc, col = "red", lwd = 2, add = TRUE)
legend("bottomright", 
       legend = c(paste("Logistic Regression (AUC =", round(logit_auc, 3), ")"),
                  paste("Decision Tree (AUC =", round(tree_auc, 3), ")")),
       col = c("blue", "red"), lwd = 2)

# AUC comparison table
auc_comparison <- data.frame(
  Model = c("Logistic Regression", "Decision Tree"),
  AUC = c(logit_auc, tree_auc)
)

auc_comparison %>%
  knitr::kable(digits = 4,
               caption = "Table 4: AUC Comparison")
```

These test figures illustrate that while performance between the models is similar, the Logistic Regression model has a slight edge in key regards.

1. The Logistic Regression had slightly higher AUC, which is similar to R-Squared in that it indicates how much of the variation is explained by each model.
2. The Logistic Regression had higher Sensitivity (Recall), which gives the designated "positive" outcome--"Survived" in this case--more weight. This is because Recall is composed of all True Positives divided by the sum of all True Positives and all False Negatives (i.e. positives incorrectly identified as negatives). In other words, Recall measures the percentage of all positive outcomes were identified as such. In the context of this dataset, Recall should be given greater consideration, because it measures models by how well they identify survivors of the sinking of the Titanic. Failing to rescue survivors would have the result of them no longer being such, as they are left in the freezing waters of the Atlantic Ocean without rescue.

### 6.2: Analysis of Champion Model Residuals

Because Logistic Models essentially take a regression model's return of a continuous value and transform it to a categorical outcome, such models require particular handling when evaluating their errors.

#### 6.2.1: Champion Model Confusion Matrix

Because the models' task is classification, predicting a binary categorical outcome, the most useful measure of "residuals" is a Confusion Matrix. Given the that the two models have comparable Confusion Matrices, the number of false positives and false negatives in the Logistic Regression model seem reasonable.

#### 6.2.2: Champion Model Residuals

Below is the plot of the model's residuals.

```{r}
plot(logit_step, which = 1)
```

In Logistic Regression models, there is nonconstant error variance because the "error" is the actual outcome value (in this case, 1 for "Survived" or 0 for "Died"), minus the predicted probability value (between 0 and 1, and seldom exactly 0 or 1). The relationship is expressed mathematically like so.

$$\epsilon_i = Y_i - \hat{\pi}_i$$

On account of this relationship, errors will not be normally distributed, and so this expectation is inapplicable. Classification of observations into one of two categories the goal. It is the application of a cutoff that transforms continuous values between 0 and 1 (inclusive) which yields the categorical outcome, and so it does not matter how far off a given raw model output value (pre-cutoff) is from 0 or 1 as long as it is on the correct side of the cutoff to match the actual value.

### 6.3: Champion Model Pseudo-R-Squared and Error Review

Because the champion model is a Logistic Regression and its challenger is a Decision Tree, only one of these models even generates errors which are appropriate for measurement with Sum of Squares of Error (SSE) or Residual Mean Squared Error (RMSE). 

However, a Pseudo-R-Squared value can be obtained using AUC, as noted above. With an **AUC value of 0.8123, the Logistic Regression has sound performance**, as it can essentially account for 81% of variation using its predictors.

### 6.4: Champion Model Relative Fit

The Logistic Regression model has a Sensitivity (another term for "Recall") of 0.692. This means that of all actual survivors, the model correctly identifies 69.2% of them. As this value is substantially better than a coin toss, this makes for an effective model.

### 6.5: Assessment of Logistic Model Assumptions

The Logistic Model does not violate most applicable assumptions.

- The model does not suffer from Multicollinearity. This was confirmed by checking Variable Inflation Factor (VIF) during model construction: it did not yield any concerningly high values (i.e. greater than 5).
- The model has sufficient data, with 1310 observations.
- The model passes linearity concerns. Most of its predictors are continuous, and its output is also continuous.

However, **it does violate the assumption of observation independence**, to a limited degree. The `family_size` predictor variable was synthesized during data cleaning, and combines...

- `sibsp`, the number of siblings or spouses aboard; and...
- `parch`, the number of parents or children aboard.

If a passenger captured in the dataset has a family member (spouse, child, or parent) aboard, and that family member is also captured in the dataset, the `family_size` value of each such passenger will increase by 1.

To understand the degree of this effect, the distribution of `family_size` values should be examined.

```{r}
has_family <- length(which(titanic_train$family_size > 0)) + length(which(titanic_test$family_size > 0))
total_pass <- nrow(titanic_train) + nrow(titanic_test)
pct_w_fam = has_family / total_pass
sprintf('Percent passengers with family aboard = %f', pct_w_fam * 100)

print('Training dataset family_size distribution:')
summary(titanic_train$family_size)
print('Testing dataset family_size distribution:')
summary(titanic_test$family_size)

boxplot(titanic_train$family_size, ylab = 'Family Size', main = 'Training Dataset')
boxplot(titanic_test$family_size, ylab = 'Family Size', main = 'Testing Dataset')
```

Nearly every passenger in the dataset has a family member aboard, meaning there is some degree of observation interdependence for this variable. However, as can be observed in the distribution of values, most passengers had between 1 and 2 family members aboard. Between the relative consistency of the data, and the fact that VIF analysis did not indicate any major issues with this (or any) variable, the violation of this assumption can be tolerated.

### 6.6: Champion Model Limitations

There are many passengers whose age is unknown. These values had to be imputed: for our analysis, observations with null age values were assigned the median age associated with their sex and class of passage aboard the Titanic. Like all imputations, this is obviously not as if all ages were truly known. 

```{r}
age_na <- length(which(is.na(Titanic.Raw$age)))
sprintf('Number of NULL age values  = %d', age_na)
sprintf('Percent of all values NULL = %f', 100 * (age_na / nrow(Titanic.Raw)))

# Citations
#   - Gemini to obtain is.na() function (original idea to use to return number of null values in column)
```

As shown above, this imputation affects about 20% of all observations.

Thus, **the Champion model assumes that the imputed values do not vary significantly from the true values, and that any such variation is evenly distributed**. Because at the time, ages in a population were roughly normally distributed, this is a reasonable-enough assumption.

### 6.7: Champion Model Required Assumptions

Logistic Regression models are very resilient because they do not require many assumptions. Such models only assume that the response variable is categorical and only has 2 states. As the variable to be predicted is whether a Titanic passenger survived or perished after the vessel's sinking, this assumption is met.

The only assumptions required to support this model pertain to the data itself, and would thus affect the Challenger model as well. Namely, those assumptions are...

1. The interrelation between observations as regards `family_size` is consistent enough that it does not seriously undermine use of this variable, and...
2. Actual ages by sex and passage class are evenly distributed enough around median ages by sex and passage class that imputing the latter for null values does not seriously undermine use of this variable.
