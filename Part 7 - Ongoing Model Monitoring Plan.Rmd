---
title: "HARVARD EXTENSION SCHOOL"
subtitle: "EXT CSCI E-106 Statistical Data Modeling Class Group Project Template"
author:
- Author One
- Author Two
- Author Three
- Author Four
- Author Five
- Author Six
- Author Seven
- Author Eight
- Author Nine

tags: [logistic, neuronal networks, etc..]
abstract: |
  This is the location for your abstract.

  It must consist of two paragraphs.
date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: margin=1.3cm
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
editor_options: 
  markdown: 
    wrap: 72
---
\newpage
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
Titanic.Data<-read.csv("Titanic_Survival_Data.csv")
```
\newpage
## Classify whether a passenger on board the maiden voyage of the RMS Titanic in 1912 survived given their age, sex and class. Sample-Data-Titanic-Survival.csv to be used in the Final Project


| Variable| Description |
| :-------:| :------- |
| pclass| **Passanger Class, could be 1st, 2nd or 3rd**    |
| survived| *Survival Status: 0=No, 1=Yes*    |
| name| *Name of the Passanger*    |
| Sex| *Sex*    |
| sibsp| *Number of Siblings or Spouses aboard*    |
| parch| *Number of Parents or Children aboard*    |
| ticket| *Ticket Number*    |
| fare| *Passenger Fare*    |
| cabin| *Cabin number, “C85” would mean the cabin is on deck C and is numbered 85.*    |
| embarked| *Port of Embarkation: C=Cherburg, S=Southampton, Q=Queenstown*    |
| boat| *Lifeboat ID, if passenger survived*    |
| body| *Body number (if passenger did not survive and body was recovered*    |
| home.dest| *The intended home destination of the passenger*    | 

\newpage
## Instructions:
0.  Join a team with your fellow students with appropriate size (Up to Nine Students total)
If you have not group by the end of the week of April 11 you may present the project by yourself or I will randomly assign other stranded student to your group. I will let know the final groups in April 11.
1.  Load and Review the dataset named "Titanic_Survival_Data.csv"
2.	Create a model development document that describes the model using this template, input the name of the authors, Harvard IDs, the name of the Group, all of your code and calculations, etc..:

## Due Date: December 15 2025 1159 pm hours EST

**Notes**
**No typographical errors, grammar mistakes, or misspelled words, use English language**
**All tables need to be numbered and describe their content in the body of the document**
**All figures/graphs need to be numbered and describe their content**
**All results must be accurate and clearly explained for a casual reviewer to fully understand their purpose and impact**
**Submit both the RMD markdown file and PDF with the sections with appropriate explanations. A more formal document in Word can be used in place of the pdf file but must include all appropriate explanations.**

Executive Summary

This section will describe the model usage, your conclusions and any regulatory and internal requirements. In a real world scneario, this section is for senior management who do not need to know the details. They need to know high level (the purpose of the model, limitations of the model and any issues).


\newpage
## I. Introduction (5 points)

*This section needs to introduce the reader to the problem to be resolved, the purpose, and the scope of the statistical testing applied. What you are doing with your prediction? What is the purpose of the model? What methods were trained on the data, how large is the test sample, and how did you build the model?*


\newpage
## I. Description of the data and quality (15 points)

*Here you need to review your data, the statistical test applied to understand the predictors and the response and how are they correlated. Extensive graph analysis is recommended. Is the data continuous, or categorical, do any transformation needed? Do you need dummies? *


\newpage
## III. Model Development Process (15 points)

*Build an appropriate model to predict probability of survival.  And of course,  create the train data set which contains 70% of the data and use set.seed (1023). The remaining 30% will be your test data set. Investigate the data and combine the level of categorical variables if needed and drop variables. For example, you can passenger name, cabin, etc.. *

\newpage
## IV. Model Performance Testing (15 points)

*Use the test data set to assess the model performances. Here, build the best model by using appropriate selection method. You may compare the performance of the best two logistic or other classification model selected. Apply remedy measures as applicable (transformation, etc.) that helps satisfy the assumptions of your particular model. Deeply investigate unequal variances and multicollinearity if warranted.  *


\newpage
## V. Challenger Models (15 points)

*Build an alternative model based on one of the following approaches to predict survival as applicable:logistic regression, decision tree, NN, or SVM, Poisson regression or negative binomial. Check the applicable model assumptions. Apply in-sample and out-of-sample testing, back testing and review the comparative goodness of fit of the candidate models. Describe step by step your procedure to get to the best model and why you believe it is fit for purpose.*

\newpage
## VI. Model Limitation and Assumptions (15 points)

*Based on the performances on both train and test data sets, determine your primary (champion) model and the other model which would be your benchmark model. Validate your models using the test sample. Do the residuals look normal? Does it matter given your technique? How is the prediction performance using Pseudo R^2, SSE, RMSE?  Benchmark the model against alternatives. How good is the relative fit? Are there any serious violations of the logistic model assumptions? Has the model had issues or limitations that the user must know? (Which assumptions are needed to support the Champion model?)* 


\newpage
## VII. Ongoing Model Monitoring Plan (5 points)

*How would you picture the model needing to be monitored, which quantitative thresholds and triggers would you set to decide when the model needs to be replaced? What are the assumptions that the model must comply with for its continuous use?*

### 7.1: Baseline Performance Metrics

The champion model (Logistic Regression) was tested on the test data set. The baseline performance values from Part IV and Part VI are shown below. These values will be used as reference points for monitoring.

```{r baseline_metrics}
# Get baseline metrics from test data
baseline_accuracy <- as.numeric(logit_cm$overall["Accuracy"])
baseline_auc <- as.numeric(logit_auc)
baseline_sensitivity <- as.numeric(logit_cm$byClass["Sensitivity"])

cat("Baseline Test Accuracy:", round(baseline_accuracy, 4), "\n")
cat("Baseline Test AUC:", round(baseline_auc, 4), "\n")
cat("Baseline Test Sensitivity:", round(baseline_sensitivity, 4), "\n")
```

**Interpretation:** These are the baseline values from the test data. We will monitor the model to make sure performance stays close to these values.

### 7.2: Performance Monitoring Thresholds

We need to set thresholds to know when the model performance is getting worse. If performance drops too much, we should retrain the model.

```{r monitoring_thresholds}
# Calculate thresholds
accuracy_warning <- baseline_accuracy - 0.05
accuracy_critical <- baseline_accuracy - 0.10

auc_warning <- baseline_auc - 0.05
auc_critical <- baseline_auc - 0.10

cat("Accuracy Monitoring:\n")
cat("  Baseline:", round(baseline_accuracy, 4), "\n")
cat("  Warning if below:", round(accuracy_warning, 4), "\n")
cat("  Critical if below:", round(accuracy_critical, 4), "\n\n")

cat("AUC Monitoring:\n")
cat("  Baseline:", round(baseline_auc, 4), "\n")
cat("  Warning if below:", round(auc_warning, 4), "\n")
cat("  Critical if below:", round(auc_critical, 4), "\n")
```

**Monitoring Plan:**
- Check model performance every month
- If accuracy drops below the warning threshold, investigate the problem
- If accuracy drops below the critical threshold, retrain the model
- Same rules apply for AUC

### 7.3: Data Drift Monitoring

Data drift happens when the input variables change over time. For example, if the age distribution of passengers changes, the model might not work as well.

```{r data_drift}
# Check baseline distributions
cat("Baseline Variable Distributions (from training data):\n\n")

cat("Passenger Class Distribution:\n")
print(round(prop.table(table(titanic_train$pclass)) * 100, 1))

cat("\nSex Distribution:\n")
print(round(prop.table(table(titanic_train$sex)) * 100, 1))

cat("\nAge Statistics:\n")
cat("  Mean:", round(mean(titanic_train$age, na.rm = TRUE), 1), "\n")
cat("  Median:", round(median(titanic_train$age, na.rm = TRUE), 1), "\n")
```

**Monitoring Plan:**
- Check variable distributions every 3 months
- If any variable distribution changes by more than 10%, investigate
- If multiple variables change, consider retraining the model

### 7.4: Model Assumptions

The logistic regression model needs certain assumptions to work properly. These must be checked regularly.

**Required Assumptions:**

1. **No Multicollinearity:** VIF values should stay below 5 (checked in Part IV)
   - Check every 3 months
   - If VIF goes above 5, investigate the problem

2. **Linearity:** Continuous variables should have a linear relationship with log-odds
   - Check every 3 months using Box-Tidwell test
   - If assumption is violated, consider transformations

3. **Independence:** Observations should be independent (some interdependence from family_size is acceptable, as noted in Part VI)
   - Monitor family_size distribution quarterly
   - If distribution changes significantly, review the model

4. **Data Quality:** 
   - Age missingness should stay below 25% (baseline is about 20%)
   - Age imputation method should remain valid
   - Check quarterly

### 7.5: When to Replace the Model

The model should be retrained or replaced if:

1. **Performance Drops:**
   - Accuracy drops below critical threshold for 3 months in a row
   - AUC drops below critical threshold for 3 months in a row
   - Sensitivity drops significantly (this is important for identifying survivors)

2. **Data Changes:**
   - Multiple variables show significant drift
   - Age distribution changes by more than 10 years
   - Missing data rate increases substantially

3. **Assumption Violations:**
   - VIF values go above 10
   - Linearity assumption is violated
   - Data quality issues cannot be fixed

### 7.6: Monitoring Schedule

```{r schedule}
monitoring_table <- data.frame(
  What_to_Monitor = c("Model Accuracy and AUC", 
                      "Variable Distributions",
                      "VIF Values",
                      "Data Quality"),
  How_Often = c("Monthly", "Every 3 months", "Every 3 months", "Every 3 months")
)

monitoring_table %>%
  knitr::kable(caption = "Table 1: Monitoring Schedule")
```

### 7.7: Summary

This monitoring plan helps ensure the champion model continues to work well. The key points are:

1. Monitor accuracy and AUC monthly - if they drop too much, investigate or retrain
2. Check data distributions every 3 months - if they change, the model may need updating
3. Verify model assumptions regularly - if assumptions are violated, fix the problem
4. Retrain the model when performance drops significantly or data changes substantially

By following this plan, we can catch problems early and keep the model performing well over time.

\newpage
## VIII. Conclusion (5 points)

*Summarize your results here. What is the best model for the data and why?*

## Bibliography (7 points)

*Please include all references, articles and papers in this section.*

## Appendix (3 points)

*Please add any additional supporting graphs, plots and data analysis.*


