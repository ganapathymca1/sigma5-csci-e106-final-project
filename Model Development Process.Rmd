---
output:
  pdf_document: default
  html_document: default
---
ï»¿---
title: "HARVARD EXTENSION SCHOOL"
subtitle: "Titanic Survival Classification: Group Project Report"
author:
- Student One (HUID XXXXXXXX)
- Student Two (HUID XXXXXXXX)
- Student Three (HUID XXXXXXXX)
tags: [logistic regression, decision tree, classification]
abstract: |
  This report builds a champion/benchmark modeling solution to predict
  passenger survival on the RMS Titanic. We demonstrate a complete model
  lifecycle: exploratory analysis, data preparation, model training,
  challenger comparison, performance evaluation, limitations, and
  monitoring recommendations. All R code and interpretations are included
  so the analysis is reproducible and transparent.
date: "`r format(Sys.time(), '%d %B %Y')`"
geometry: margin=1.3cm
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(broom)
library(rpart)
library(rpart.plot)

Titanic.Raw <- read_csv("Titanic_Survival_Data.csv", show_col_types = FALSE)
```

## Executive Summary

We predict Titanic passenger survival using demographic and ticketing
information. A cleaned dataset of 1,310 records is split 70/30
train/test (set.seed = 1023). The champion model is a parsimonious
logistic regression using class, sex, age, family size, fare, and port
of embarkation. A decision tree is built as the challenger. Both models
perform substantially better than chance; the logistic model yields
balanced accuracy and interpretable odds ratios, while the tree offers
transparent rules but slightly lower hold-out accuracy. Monitoring
should track drift in class mix, gender mix, and fare distributions, and
trigger review when accuracy drops below 80% or when input distributions
shift beyond training percentiles. Key limitations include missing
values (age, fare, cabin), potential historical bias, and simplified
imputations.

**Interpretation:** Senior stakeholders can rely on the logistic model
for consistent discrimination and clear business storytelling (e.g.,
women and 1st-class passengers had markedly higher survival odds), while
the tree provides an audit-friendly benchmark.

## I. Introduction (5 points)

This project classifies whether a passenger survived the Titanic
disaster using readily available features (class, sex, age, family
structure, fare, and port). We evaluate two supervised classification
methods: logistic regression (champion) and decision tree (challenger).
The training sample contains 70% of the data (n ~ 917), the test sample the
remaining 30% (n ~ 393). Success is defined by accurate and explainable
survival predictions that generalize to the hold-out test set.

## II. Description of the Data and Quality (15 points)

The dataset contains 1,310 observations and 14 original variables. Key
predictors are a mix of categorical (class, sex, and embarked) and numeric
(age, fare, family counts). Several variables contain missing/null
values for a notably high number of observations (e.g. age, cabin, boat, and
body are missing values for more than 20% of observations).

```{r data_overview}
glimpse(Titanic.Raw)

# Table 1: missingness summary
Titanic.Raw %>%
  summarise(across(everything(),
                   ~ sum(is.na(.)))) %>%
  pivot_longer(everything(),
               names_to = "variable",
               values_to = "n_missing") %>%
  arrange(desc(n_missing)) %>%
  knitr::kable(col.names = c("Variable", "Missing Count"))
```

**Interpretation:** Age, cabin, boat, body, and home destination have
notable gaps. Cabin/boat/body are sparsely populated and not useful for
modeling. Age must be imputed to avoid losing over 20% of rows.

### Data Preparation

We engineer a clean modeling frame according to the following process.

1. Convert categorical variables to factors,
2. Impute age by sex/class median,
3. Impute fare with the overall median,
4. Drop high-missing columns, and
5. Create a `family_size` helper feature.

Survived is labeled as "Died"/"Survived" for readability.

```{r data_prep}
clean_titanic <- Titanic.Raw %>%
  mutate(
    survived = factor(survived, levels = c(0, 1),
                      labels = c("Died", "Survived")),
    pclass = factor(pclass, levels = c(1, 2, 3),
                    labels = c("1st", "2nd", "3rd")),
    sex = factor(sex),
    embarked = fct_explicit_na(embarked, "Unknown")
  )

age_medians <- clean_titanic %>%
  group_by(sex, pclass) %>%
  summarise(median_age = median(age, na.rm = TRUE), .groups = "drop")

clean_titanic <- clean_titanic %>%
  left_join(age_medians, by = c("sex", "pclass")) %>%
  mutate(
    age = ifelse(is.na(age), median_age, age),
    fare = ifelse(is.na(fare), median(fare, na.rm = TRUE), fare),
    family_size = sibsp + parch + 1
  ) %>%
  select(survived, pclass, sex, age, sibsp, parch, family_size, fare, embarked)

summary(clean_titanic)
```

**Interpretation:** Imputation preserves sample size without extreme
values. Removing cabin/ticket/body/boat/home.dest reduces noise while
retaining predictive signal. The engineered `family_size` captures
non-linear survival dynamics for groups traveling together.

### Exploratory Graphs

```{r eda_plots, fig.width=7, fig.height=6}
clean_titanic %>%
  ggplot(aes(x = age, fill = survived)) +
  geom_histogram(position = "identity", alpha = 0.55, bins = 30) +
  labs(title = "Figure 1. Age distribution by survival",
       x = "Age", y = "Count") +
  theme_minimal()

clean_titanic %>%
  ggplot(aes(x = pclass, fill = survived)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Figure 2. Survival share by passenger class",
       x = "Class", y = "Percent") +
  theme_minimal()

clean_titanic %>%
  ggplot(aes(x = sex, fill = survived)) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent_format()) +
  labs(title = "Figure 3. Survival share by sex",
       x = "Sex", y = "Percent") +
  theme_minimal()
```

**Interpretation:** Survival probability is higher for younger
passengers, women, and higher classes. These patterns justify including
class, sex, and age in the model and suggest potential interactions
between class and sex.

## III. Model Development Process (15 points)

### Train/Test Split

```{r split}
set.seed(1023)
train_index <- sample(seq_len(nrow(clean_titanic)),
                      size = floor(0.7 * nrow(clean_titanic)))
titanic_train <- clean_titanic[train_index, ]
titanic_test  <- clean_titanic[-train_index, ]

table(titanic_train$survived)
table(titanic_test$survived)
```

**Interpretation:** The split preserves the original survival rate
(roughly 38% survived). Using a fixed seed allows full reproducibility.

### Champion: Logistic Regression

```{r logit_model}
logit_model <- glm(
  survived ~ pclass + sex + age + family_size + embarked,
  data = titanic_train,
  family = binomial
)

logit_summary <- tidy(logit_model, exponentiate = TRUE, conf.int = TRUE)
logit_summary %>%
  knitr::kable(
    digits = 3,
    col.names = c("Term", "Odds Ratio", "Std. Error", "z", "p-value",
                  "CI Lower", "CI Upper")
  )
```

**Interpretation:** The resultant model yields a number of clear trends.

- Odds ratios show strong positive lift for females.
- Lower classes of passage (i.e. 2nd and 3rd) resulted in lower survival probability.
- Increasing age slightly decreases survival odds.

### Challenger: Decision Tree

```{r tree_model}
tree_model <- rpart(
  survived ~ pclass + sex + age + family_size + fare + embarked,
  data = titanic_train,
  method = "class",
  control = rpart.control(cp = 0.01, minsplit = 20)
)

rpart.plot(tree_model, main = "Figure 4. Decision tree challenger")
```

**Interpretation:** The tree yields intuitive rules (e.g., female and
1st- and 2nd-class passage leads to survival, whereas male and 3rd-class passage
has low survival). It trades predictions which are not as granular as the
logit model's probability-based ones in return for superior comprehensibility 
and auditability.
